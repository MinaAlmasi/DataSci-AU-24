{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class 02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "%pip install pandas matplotlib seaborn scikit-learn numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib \n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = pathlib.Path.cwd()\n",
    "datapath = path.parents[4] / \"data\" / \"class_01\" / \"bikes.csv\"\n",
    "df = pd.read_csv(datapath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXERCISES\n",
    "1. Create a folder called `group-x` within `nbs/class_02`, `cd` into it and work within that today\n",
    "2. Choose an outcome variable for a regression problem. On the basis of this, define **which of the evaluation metrics** could be suitable. Evaluation metrics can be computed using scikit-learn: https://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics \n",
    "3. (a) If you are in the bike sharing group, split your dataset into a training/validation/test set using later time points as validation/test set. Validation and test set should be 15% of your data each. (b) If you are in the personality group, using sklearn's `train_test_split` function, create a 70/15/15 random split of your data.\n",
    "    - Remember to set a seed (`random_state`) when you do so. Let's all use the same (the classic `random_state=42`)\n",
    "    - Save these datasets as separate csv files in a subfolder called `data`\n",
    "4. Look at your outcome and predictors: do you want to transform them in any way?\n",
    "5. Estimate the performance of a dummy baseline (i.e., the mean model) on all splits\n",
    "6. Now look at your predictors: do they need any preprocessing? Any transformations? Removal of \"bad\" data points?\n",
    "7. Fit the other models using KKN (sklearn's `KNeighborsRegressor`: https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html) and linear models (`LinearRegressor`: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html). Save the fitted model object (with a meaningful name) using `pickle` (https://scikit-learn.org/stable/model_persistence.html) in a subfolder called `model`.\n",
    "8. Once you are done, evaluate all models on both the training and the validation set and visualize the scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creating an Outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"proportion_casual_reg\"] = df[\"casual\"]/df[\"cnt\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which evaluation metrics would be suitable? We'll chose R-squared as we are interested in a general metric on how good our model fits to the data and not necessarily interested in how our model fares on individual data points. Therefore it would not make sense to use RMSE/MSE as they are scale-dependent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Splitting our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_df = len(df)\n",
    "\n",
    "# define the split percentage\n",
    "split_size = 0.15\n",
    "\n",
    "# get the absolute number of rows that equals to our split size (use int to rm. decimal)\n",
    "n_rows = int(len_df * split_size)\n",
    "\n",
    "# define test\n",
    "test_df = df.iloc[-n_rows:, :]\n",
    "\n",
    "# define train and val combined\n",
    "train_val_df = df.iloc[:-n_rows, :]\n",
    "\n",
    "# subset train from only the train and val \n",
    "train_df = train_val_df.iloc[:-n_rows, :]\n",
    "\n",
    "# subset val from only the train and val\n",
    "val_df = train_val_df.iloc[-n_rows:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dataset\n",
    "save_path = path.parents[2] / \"nbs\" / \"class_02\" / \"group-RMDS\" / \"data\"\n",
    "save_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "train_df.to_csv(save_path / \"train_bikes.csv\", index=False)\n",
    "val_df.to_csv(save_path / \"val_bikes.csv\", index=False)\n",
    "test_df.to_csv(save_path / \"test_bikes.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into predictors and features\n",
    "predictors = [\"proportion_casual_reg\", \"registered\", \"casual\", \"cnt\", \"dteday\"]\n",
    "X_train = train_df.drop(predictors, axis=1)\n",
    "y_train = train_df[\"proportion_casual_reg\"].values\n",
    "\n",
    "X_val = val_df.drop(predictors, axis=1)\n",
    "y_val = val_df[\"proportion_casual_reg\"].values\n",
    "\n",
    "X_test = test_df.drop(predictors, axis=1)\n",
    "y_test = test_df[\"proportion_casual_reg\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "performances = []\n",
    "\n",
    "mean_value = y_train.mean()\n",
    "model_name = 'dummy'\n",
    "\n",
    "for y, nsplit in zip([y_train, y_val, y_test],\n",
    "                    ['train', 'val', 'test']):\n",
    "    performance = np.sqrt(mean_squared_error(y, \n",
    "                                             [mean_value]*y.shape[0]))\n",
    "    r2 = r2_score(y, [mean_value]*y.shape[0])\n",
    "    performances.append({'model': model_name,\n",
    "                         'split': nsplit,\n",
    "                         'rmse': performance.round(4),\n",
    "                         'r2': r2.round(4)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'model': 'dummy', 'split': 'train', 'rmse': 0.1426, 'r2': 0.0},\n",
       " {'model': 'dummy', 'split': 'val', 'rmse': 0.1224, 'r2': -0.0753},\n",
       " {'model': 'dummy', 'split': 'test', 'rmse': 0.12, 'r2': -0.0734}]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performances"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
