{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class 5 - Bagging and Boosting\n",
    "\n",
    "### Recap of lecture and introductory remarks\n",
    "In yesterday's lecture, we introduced bagging and boosting as two techniques to reduce variance and reduce variance of decision trees. Bagging and boosting are not specific to decision trees, but we will see them in action with this kind of model.\n",
    "\n",
    "We used bagging to train a set of small decision trees (weak learners) on subsets of the training data, whose individual predictions we aggregate to make a single prediction. The resulting model is an _ensemble_ model. We have seen that `Random Forests` are popular learning algorithms that combine bagging with random sampling of features, to induce diversity in decision trees and further regularization.\n",
    "\n",
    "On the other hand, boosting consists in training a sequence of decision trees which iteratively reduce the error of the previous decision tree because they are fitted on the residuals or on the gradients of the previous tree. We have focused specifically on `gradient boosting` and indicated `XGBoost` as a particularly powerful implementation of boosting + bagging.\n",
    "\n",
    "Today, we will go back to the bike data, and fit `RandomForest` and `XGBoost` models, comparing their performances to those of models fitted previously. We will implement Random Forests using `scikit-learn`, and XGBoost using the XGBoost package: https://xgboost.readthedocs.io/en/stable/ \n",
    "\n",
    "**Note**: As last week, under `nbs/class_05` you will find a notebook called `example.ipynb`, where I provide an example of how to run today's exercise on sample data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operational remarks\n",
    "Two suggestions on how to go about this, based on where you are at regarding exercises from previous weeks.\n",
    "\n",
    "1. If you have done exercises from class 2 and 3, you will have one/two notebooks with baseline, linear, KNN, and linear regularized models, as well as records of performances (which will be handy to compare performances of our new methods). In this case, my suggestion would be to work on a new notebook where you only fit the new models, and load the performance of the old models for comparison.\n",
    "\n",
    "2. If you have not done exercises from previous classes, you have three options:\n",
    "- Work on a new notebook where you only fit the models we work on today (random forest and XGBoost). Optionally, you can \"manually\" compare the performance of your new models to plots from previous weeks\n",
    "- Work on a new notebook and also implement a couple of models from previous weeks for comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Today's exercise\n",
    "Work in groups on the following tasks\n",
    "\n",
    "1. Fit a `Random Forest` model to the data (https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html#sklearn.ensemble.RandomForestRegressor), using cross-validation to define the best possible range of parameters\n",
    "    - There are a number of parameters that should be passed to the estimator. Carefully read the documentation, and identify a few hyperparameters you might want to manipulate\n",
    "    - Define a series of possible values for these hyperparameters, and store this information into a Python dictionary. For each hyperparameter, the dictionary should include the name of the hyperparameters (as a string) as `key`, and a list including a range of possible values as `value`\n",
    "    - Pass your estimator and the parameter grid to `GridSearchCV`: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html and fit this object to your training set. If you have defined *a lot* of possible values, you can consider using `RandomizedSearchCV`: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html. **Note**: you need to pass something appropriate as the value of the `scoring` argument\n",
    "    - Try to answer the following questions:\n",
    "        - What is `GridSearchCV` doing?\n",
    "        - What is the difference between `RandomizedSearchCV` and `GridSearchCV`?\n",
    "        - **Bonus question**: Given that we do have a validation set, could we do model selection without using cross-validation? Which parameter in `GridSearchCV` or `RandomizedSearchCV` would you have to change, and how, to do so?\n",
    "    - Find out which hyperparameters gave the best result\n",
    "        - **Hint**: look at the `.best_estimator_` attribute on a fitted `GridSearchCV`/`RandomizedSearchCV` and `.get_params()`\n",
    "    - Compute the performance of this model on the training, validation, and test set\n",
    "    - Compute and plot feature importances for the resulting model. You can look at the `.feature_importances_` attribute of the best estimator.\n",
    "        - **Bonus question**: which method is used by default to compute feature importances? Is any other method available in `sklearn`?\n",
    "\n",
    "2. Do the exact same things as 1., this time using `XGBRegressor` (https://xgboost.readthedocs.io/en/stable/python/python_api.html#xgboost.XGBRegressor)\n",
    "    - Note: you will have to install `xgboost` (https://xgboost.readthedocs.io/en/stable/install.html) to run this (in short `pip install xgboost`)\n",
    "    - You will have to define an appropriate `scoring` parameter\n",
    "    - Parameters for grid/randomized search will be slightly different: look at the documentation for XGBRegressor, and make informed choices based on what we discussed in class\n",
    "\n",
    "3. Plot the performance of the best Random Forest models and the best XGBoost models, against models you fitted previously\n",
    "    - Which models perform best?\n",
    "    - How does the performance profile of RandomForest compare to XGBoost? Why?\n",
    "\n",
    "4. Compare feature importances across `RandomForest` and `XGBoost`: do they look similar/different?\n",
    "\n",
    "5. Overall reflection on modeling process\n",
    "    - Reflect back on your choices for previous models: should you have transformed any of the features before fitting Linear Regression, KNN, or regularized models?\n",
    "    - Can you think of ways in which our predictive problem can be made more interesting from a business perspective?\n",
    "    - Which aspect of the data are we *not* modeling, that we could/should model?\n",
    "\n",
    "\n",
    "### Extra tasks\n",
    "- Estimate a `DecisionTreeRegressor` with cross-validation, using the same logic we applied above: how does the performance of the resulting model compare to `RandomForestRegressor` and `XGBoost` regressor?\n",
    "- Go back to your fitted `GridSearchCV` or `RandomizedSearchCV`, and inspect their attributes. Can you plot performance against values of each of the parameters you are fitting? Is there any systematic pattern?\n",
    "- Reflect on hyperparameters passed to `GridSearchCV` or `RandomizedSearchCV`: how do you expect that individual manipulations of these parameters would affect the bias/variance profile of your models?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(f'group-RMDS/data/train_bikes.csv', index_col=0)\n",
    "val = pd.read_csv(f'group-RMDS/data/val_bikes.csv', index_col=0)\n",
    "test = pd.read_csv(f'group-RMDS/data/test_bikes.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['dteday', 'yr', 'hr', 'holiday', 'workingday', 'temp', 'atemp', 'hum',\n",
       "       'windspeed', 'casual', 'registered', 'cnt', 'proportion_casual_reg',\n",
       "       'season_1', 'season_2', 'season_3', 'season_4', 'mnth_1', 'mnth_2',\n",
       "       'mnth_3', 'mnth_4', 'mnth_5', 'mnth_6', 'mnth_7', 'mnth_8', 'mnth_9',\n",
       "       'mnth_10', 'mnth_11', 'mnth_12', 'weekday_0', 'weekday_1', 'weekday_2',\n",
       "       'weekday_3', 'weekday_4', 'weekday_5', 'weekday_6', 'weathersit_1',\n",
       "       'weathersit_2', 'weathersit_3', 'weathersit_4'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create X_splits \n",
    "redundant_cols = [\"proportion_casual_reg\", \"registered\", \"casual\", \"cnt\", \"dteday\", \"yr\"]\n",
    "X_train = train.drop(redundant_cols, axis=1)\n",
    "X_val = val.drop(redundant_cols, axis=1)\n",
    "X_test = test.drop(redundant_cols, axis=1)\n",
    "\n",
    "# create y_splits \n",
    "y_train = train[\"cnt\"].values\n",
    "y_val = val[\"cnt\"].values\n",
    "y_test = test[\"cnt\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "## code from 02 - 03\n",
    "def evaluate(model, X, y, nsplit, model_name, constant_value=None):\n",
    "    ''' Evaluates the performance of a model \n",
    "    Args:\n",
    "        model (sklearn.Estimator): fitted sklearn estimator\n",
    "        X (np.array): predictors\n",
    "        y (np.array): true outcome\n",
    "        nsplit (str): name of the split\n",
    "        model_name (str): string id of the model\n",
    "        constant_value (int or None): relevant if the model predicts a constant\n",
    "    '''\n",
    "    performances = []\n",
    "\n",
    "    if constant_value is not None:\n",
    "        preds = np.array([constant_value] * y.shape[0])\n",
    "    else:\n",
    "        preds = model.predict(X)\n",
    "    r2 = r2_score(y, preds)\n",
    "    performance = np.sqrt(mean_squared_error(y, preds))\n",
    "    performances.append({'model': model_name,\n",
    "                         'split': nsplit,\n",
    "                         'rmse': performance.round(4),\n",
    "                         'r2': r2.round(4)})\n",
    "\n",
    "    return performances\n",
    "\n",
    "def evaluate_across_splits(model, X_splits, y_splits, split_names, model_name, constant_value=None):\n",
    "    '''\n",
    "    Evaluates the performance of a model across different splits\n",
    "    Args:\n",
    "        model (sklearn.Estimator): fitted sklearn estimator\n",
    "        X_splits (list): list of arrays containing predictors for each split\n",
    "        y_splits (list): list of arrays containing true outcome for each split\n",
    "        split_names (list): list of split names\n",
    "        model_name (str): string id of the model\n",
    "        constant_value (int or None): relevant if the model predicts a constant\n",
    "    Returns:\n",
    "        list: list of performance metrics for each split\n",
    "    '''\n",
    "    performances = []\n",
    "\n",
    "    for X, y, split_name in zip(X_splits, y_splits, split_names):\n",
    "        performance_metrics = evaluate(model, X, y, split_name, model_name, constant_value)\n",
    "        performances.extend(performance_metrics)\n",
    "\n",
    "    return performances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests on the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"n_estimators\": [10, 50, 100, 200], # number of trees\n",
    "    \"criterion\": [\"squared_error\", \"absolute_error\"], # fn to measure quality of split\n",
    "    \"max_depth\": [2, 5, 8, None], # max depth of tree \n",
    "    \"min_samples_split\": [2, 0.1, 0.3], # min. number of samples required to split a node. default of 2 (int = amount or float = percentage)\n",
    "}\n",
    "\n",
    "# init estimator  \n",
    "estimator = RandomForestRegressor(random_state=3)\n",
    "\n",
    "# init grid search obj.\n",
    "grid = GridSearchCV(estimator, param_grid=params, cv=5, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n",
      "[CV] END criterion=squared_error, max_depth=2, min_samples_split=2, n_estimators=10; total time=   0.1s\n",
      "[CV] END criterion=squared_error, max_depth=2, min_samples_split=2, n_estimators=10; total time=   0.1s\n",
      "[CV] END criterion=squared_error, max_depth=2, min_samples_split=2, n_estimators=10; total time=   0.1s\n",
      "[CV] END criterion=squared_error, max_depth=2, min_samples_split=2, n_estimators=10; total time=   0.1s\n",
      "[CV] END criterion=squared_error, max_depth=2, min_samples_split=2, n_estimators=10; total time=   0.1s\n",
      "[CV] END criterion=squared_error, max_depth=2, min_samples_split=2, n_estimators=50; total time=   0.3s\n",
      "[CV] END criterion=squared_error, max_depth=2, min_samples_split=2, n_estimators=50; total time=   0.3s\n",
      "[CV] END criterion=squared_error, max_depth=2, min_samples_split=2, n_estimators=50; total time=   0.3s\n",
      "[CV] END criterion=squared_error, max_depth=2, min_samples_split=2, n_estimators=50; total time=   0.3s\n",
      "[CV] END criterion=squared_error, max_depth=2, min_samples_split=2, n_estimators=50; total time=   0.3s\n",
      "[CV] END criterion=squared_error, max_depth=2, min_samples_split=2, n_estimators=100; total time=   0.6s\n",
      "[CV] END criterion=squared_error, max_depth=2, min_samples_split=2, n_estimators=100; total time=   0.5s\n",
      "[CV] END criterion=squared_error, max_depth=2, min_samples_split=2, n_estimators=100; total time=   0.6s\n",
      "[CV] END criterion=squared_error, max_depth=2, min_samples_split=2, n_estimators=100; total time=   0.6s\n",
      "[CV] END criterion=squared_error, max_depth=2, min_samples_split=2, n_estimators=100; total time=   0.6s\n",
      "[CV] END criterion=squared_error, max_depth=2, min_samples_split=2, n_estimators=200; total time=   1.2s\n",
      "[CV] END criterion=squared_error, max_depth=2, min_samples_split=2, n_estimators=200; total time=   1.1s\n",
      "[CV] END criterion=squared_error, max_depth=2, min_samples_split=2, n_estimators=200; total time=   1.2s\n",
      "[CV] END criterion=squared_error, max_depth=2, min_samples_split=2, n_estimators=200; total time=   1.2s\n",
      "[CV] END criterion=squared_error, max_depth=2, min_samples_split=2, n_estimators=200; total time=   1.2s\n",
      "[CV] END criterion=squared_error, max_depth=2, min_samples_split=0.1, n_estimators=10; total time=   0.1s\n",
      "[CV] END criterion=squared_error, max_depth=2, min_samples_split=0.1, n_estimators=10; total time=   0.1s\n",
      "[CV] END criterion=squared_error, max_depth=2, min_samples_split=0.1, n_estimators=10; total time=   0.1s\n",
      "[CV] END criterion=squared_error, max_depth=2, min_samples_split=0.1, n_estimators=10; total time=   0.1s\n",
      "[CV] END criterion=squared_error, max_depth=2, min_samples_split=0.1, n_estimators=10; total time=   0.1s\n",
      "[CV] END criterion=squared_error, max_depth=2, min_samples_split=0.1, n_estimators=50; total time=   0.3s\n",
      "[CV] END criterion=squared_error, max_depth=2, min_samples_split=0.1, n_estimators=50; total time=   0.3s\n",
      "[CV] END criterion=squared_error, max_depth=2, min_samples_split=0.1, n_estimators=50; total time=   0.3s\n",
      "[CV] END criterion=squared_error, max_depth=2, min_samples_split=0.1, n_estimators=50; total time=   0.3s\n",
      "[CV] END criterion=squared_error, max_depth=2, min_samples_split=0.1, n_estimators=50; total time=   0.3s\n",
      "[CV] END criterion=squared_error, max_depth=2, min_samples_split=0.1, n_estimators=100; total time=   0.6s\n",
      "[CV] END criterion=squared_error, max_depth=2, min_samples_split=0.1, n_estimators=100; total time=   0.6s\n",
      "[CV] END criterion=squared_error, max_depth=2, min_samples_split=0.1, n_estimators=100; total time=   0.6s\n",
      "[CV] END criterion=squared_error, max_depth=2, min_samples_split=0.1, n_estimators=100; total time=   0.6s\n",
      "[CV] END criterion=squared_error, max_depth=2, min_samples_split=0.1, n_estimators=100; total time=   0.6s\n",
      "[CV] END criterion=squared_error, max_depth=2, min_samples_split=0.1, n_estimators=200; total time=   1.2s\n",
      "[CV] END criterion=squared_error, max_depth=2, min_samples_split=0.1, n_estimators=200; total time=   1.1s\n",
      "[CV] END criterion=squared_error, max_depth=2, min_samples_split=0.1, n_estimators=200; total time=   1.2s\n",
      "[CV] END criterion=squared_error, max_depth=2, min_samples_split=0.1, n_estimators=200; total time=   1.2s\n",
      "[CV] END criterion=squared_error, max_depth=2, min_samples_split=0.1, n_estimators=200; total time=   1.2s\n",
      "[CV] END criterion=squared_error, max_depth=2, min_samples_split=0.3, n_estimators=10; total time=   0.1s\n",
      "[CV] END criterion=squared_error, max_depth=2, min_samples_split=0.3, n_estimators=10; total time=   0.1s\n",
      "[CV] END criterion=squared_error, max_depth=2, min_samples_split=0.3, n_estimators=10; total time=   0.1s\n",
      "[CV] END criterion=squared_error, max_depth=2, min_samples_split=0.3, n_estimators=10; total time=   0.1s\n",
      "[CV] END criterion=squared_error, max_depth=2, min_samples_split=0.3, n_estimators=10; total time=   0.1s\n",
      "[CV] END criterion=squared_error, max_depth=2, min_samples_split=0.3, n_estimators=50; total time=   0.3s\n",
      "[CV] END criterion=squared_error, max_depth=2, min_samples_split=0.3, n_estimators=50; total time=   0.3s\n",
      "[CV] END criterion=squared_error, max_depth=2, min_samples_split=0.3, n_estimators=50; total time=   0.3s\n",
      "[CV] END criterion=squared_error, max_depth=2, min_samples_split=0.3, n_estimators=50; total time=   0.3s\n",
      "[CV] END criterion=squared_error, max_depth=2, min_samples_split=0.3, n_estimators=50; total time=   0.3s\n",
      "[CV] END criterion=squared_error, max_depth=2, min_samples_split=0.3, n_estimators=100; total time=   0.5s\n",
      "[CV] END criterion=squared_error, max_depth=2, min_samples_split=0.3, n_estimators=100; total time=   0.5s\n",
      "[CV] END criterion=squared_error, max_depth=2, min_samples_split=0.3, n_estimators=100; total time=   0.5s\n",
      "[CV] END criterion=squared_error, max_depth=2, min_samples_split=0.3, n_estimators=100; total time=   0.5s\n",
      "[CV] END criterion=squared_error, max_depth=2, min_samples_split=0.3, n_estimators=100; total time=   0.5s\n",
      "[CV] END criterion=squared_error, max_depth=2, min_samples_split=0.3, n_estimators=200; total time=   1.0s\n",
      "[CV] END criterion=squared_error, max_depth=2, min_samples_split=0.3, n_estimators=200; total time=   1.0s\n",
      "[CV] END criterion=squared_error, max_depth=2, min_samples_split=0.3, n_estimators=200; total time=   1.0s\n",
      "[CV] END criterion=squared_error, max_depth=2, min_samples_split=0.3, n_estimators=200; total time=   1.0s\n",
      "[CV] END criterion=squared_error, max_depth=2, min_samples_split=0.3, n_estimators=200; total time=   1.0s\n",
      "[CV] END criterion=squared_error, max_depth=5, min_samples_split=2, n_estimators=10; total time=   0.1s\n",
      "[CV] END criterion=squared_error, max_depth=5, min_samples_split=2, n_estimators=10; total time=   0.1s\n",
      "[CV] END criterion=squared_error, max_depth=5, min_samples_split=2, n_estimators=10; total time=   0.2s\n",
      "[CV] END criterion=squared_error, max_depth=5, min_samples_split=2, n_estimators=10; total time=   0.1s\n",
      "[CV] END criterion=squared_error, max_depth=5, min_samples_split=2, n_estimators=10; total time=   0.1s\n",
      "[CV] END criterion=squared_error, max_depth=5, min_samples_split=2, n_estimators=50; total time=   0.7s\n",
      "[CV] END criterion=squared_error, max_depth=5, min_samples_split=2, n_estimators=50; total time=   0.6s\n",
      "[CV] END criterion=squared_error, max_depth=5, min_samples_split=2, n_estimators=50; total time=   0.6s\n",
      "[CV] END criterion=squared_error, max_depth=5, min_samples_split=2, n_estimators=50; total time=   0.6s\n",
      "[CV] END criterion=squared_error, max_depth=5, min_samples_split=2, n_estimators=50; total time=   0.6s\n",
      "[CV] END criterion=squared_error, max_depth=5, min_samples_split=2, n_estimators=100; total time=   1.2s\n",
      "[CV] END criterion=squared_error, max_depth=5, min_samples_split=2, n_estimators=100; total time=   1.3s\n",
      "[CV] END criterion=squared_error, max_depth=5, min_samples_split=2, n_estimators=100; total time=   1.2s\n",
      "[CV] END criterion=squared_error, max_depth=5, min_samples_split=2, n_estimators=100; total time=   1.3s\n",
      "[CV] END criterion=squared_error, max_depth=5, min_samples_split=2, n_estimators=100; total time=   1.2s\n",
      "[CV] END criterion=squared_error, max_depth=5, min_samples_split=2, n_estimators=200; total time=   2.6s\n",
      "[CV] END criterion=squared_error, max_depth=5, min_samples_split=2, n_estimators=200; total time=   2.4s\n",
      "[CV] END criterion=squared_error, max_depth=5, min_samples_split=2, n_estimators=200; total time=   2.4s\n",
      "[CV] END criterion=squared_error, max_depth=5, min_samples_split=2, n_estimators=200; total time=   2.5s\n",
      "[CV] END criterion=squared_error, max_depth=5, min_samples_split=2, n_estimators=200; total time=   2.4s\n",
      "[CV] END criterion=squared_error, max_depth=5, min_samples_split=0.1, n_estimators=10; total time=   0.1s\n",
      "[CV] END criterion=squared_error, max_depth=5, min_samples_split=0.1, n_estimators=10; total time=   0.1s\n",
      "[CV] END criterion=squared_error, max_depth=5, min_samples_split=0.1, n_estimators=10; total time=   0.1s\n",
      "[CV] END criterion=squared_error, max_depth=5, min_samples_split=0.1, n_estimators=10; total time=   0.1s\n",
      "[CV] END criterion=squared_error, max_depth=5, min_samples_split=0.1, n_estimators=10; total time=   0.1s\n",
      "[CV] END criterion=squared_error, max_depth=5, min_samples_split=0.1, n_estimators=50; total time=   0.5s\n",
      "[CV] END criterion=squared_error, max_depth=5, min_samples_split=0.1, n_estimators=50; total time=   0.5s\n",
      "[CV] END criterion=squared_error, max_depth=5, min_samples_split=0.1, n_estimators=50; total time=   0.5s\n",
      "[CV] END criterion=squared_error, max_depth=5, min_samples_split=0.1, n_estimators=50; total time=   0.5s\n",
      "[CV] END criterion=squared_error, max_depth=5, min_samples_split=0.1, n_estimators=50; total time=   0.5s\n",
      "[CV] END criterion=squared_error, max_depth=5, min_samples_split=0.1, n_estimators=100; total time=   1.0s\n",
      "[CV] END criterion=squared_error, max_depth=5, min_samples_split=0.1, n_estimators=100; total time=   1.0s\n",
      "[CV] END criterion=squared_error, max_depth=5, min_samples_split=0.1, n_estimators=100; total time=   1.0s\n",
      "[CV] END criterion=squared_error, max_depth=5, min_samples_split=0.1, n_estimators=100; total time=   1.0s\n",
      "[CV] END criterion=squared_error, max_depth=5, min_samples_split=0.1, n_estimators=100; total time=   1.0s\n",
      "[CV] END criterion=squared_error, max_depth=5, min_samples_split=0.1, n_estimators=200; total time=   2.1s\n",
      "[CV] END criterion=squared_error, max_depth=5, min_samples_split=0.1, n_estimators=200; total time=   1.9s\n",
      "[CV] END criterion=squared_error, max_depth=5, min_samples_split=0.1, n_estimators=200; total time=   1.9s\n",
      "[CV] END criterion=squared_error, max_depth=5, min_samples_split=0.1, n_estimators=200; total time=   2.0s\n",
      "[CV] END criterion=squared_error, max_depth=5, min_samples_split=0.1, n_estimators=200; total time=   2.0s\n",
      "[CV] END criterion=squared_error, max_depth=5, min_samples_split=0.3, n_estimators=10; total time=   0.1s\n",
      "[CV] END criterion=squared_error, max_depth=5, min_samples_split=0.3, n_estimators=10; total time=   0.1s\n",
      "[CV] END criterion=squared_error, max_depth=5, min_samples_split=0.3, n_estimators=10; total time=   0.1s\n",
      "[CV] END criterion=squared_error, max_depth=5, min_samples_split=0.3, n_estimators=10; total time=   0.1s\n",
      "[CV] END criterion=squared_error, max_depth=5, min_samples_split=0.3, n_estimators=10; total time=   0.1s\n",
      "[CV] END criterion=squared_error, max_depth=5, min_samples_split=0.3, n_estimators=50; total time=   0.3s\n",
      "[CV] END criterion=squared_error, max_depth=5, min_samples_split=0.3, n_estimators=50; total time=   0.3s\n",
      "[CV] END criterion=squared_error, max_depth=5, min_samples_split=0.3, n_estimators=50; total time=   0.3s\n",
      "[CV] END criterion=squared_error, max_depth=5, min_samples_split=0.3, n_estimators=50; total time=   0.3s\n",
      "[CV] END criterion=squared_error, max_depth=5, min_samples_split=0.3, n_estimators=50; total time=   0.3s\n",
      "[CV] END criterion=squared_error, max_depth=5, min_samples_split=0.3, n_estimators=100; total time=   0.6s\n",
      "[CV] END criterion=squared_error, max_depth=5, min_samples_split=0.3, n_estimators=100; total time=   0.5s\n",
      "[CV] END criterion=squared_error, max_depth=5, min_samples_split=0.3, n_estimators=100; total time=   0.5s\n",
      "[CV] END criterion=squared_error, max_depth=5, min_samples_split=0.3, n_estimators=100; total time=   0.6s\n",
      "[CV] END criterion=squared_error, max_depth=5, min_samples_split=0.3, n_estimators=100; total time=   0.5s\n",
      "[CV] END criterion=squared_error, max_depth=5, min_samples_split=0.3, n_estimators=200; total time=   1.3s\n",
      "[CV] END criterion=squared_error, max_depth=5, min_samples_split=0.3, n_estimators=200; total time=   1.1s\n",
      "[CV] END criterion=squared_error, max_depth=5, min_samples_split=0.3, n_estimators=200; total time=   1.1s\n",
      "[CV] END criterion=squared_error, max_depth=5, min_samples_split=0.3, n_estimators=200; total time=   1.3s\n",
      "[CV] END criterion=squared_error, max_depth=5, min_samples_split=0.3, n_estimators=200; total time=   1.1s\n",
      "[CV] END criterion=squared_error, max_depth=8, min_samples_split=2, n_estimators=10; total time=   0.2s\n",
      "[CV] END criterion=squared_error, max_depth=8, min_samples_split=2, n_estimators=10; total time=   0.2s\n",
      "[CV] END criterion=squared_error, max_depth=8, min_samples_split=2, n_estimators=10; total time=   0.2s\n",
      "[CV] END criterion=squared_error, max_depth=8, min_samples_split=2, n_estimators=10; total time=   0.2s\n",
      "[CV] END criterion=squared_error, max_depth=8, min_samples_split=2, n_estimators=10; total time=   0.2s\n",
      "[CV] END criterion=squared_error, max_depth=8, min_samples_split=2, n_estimators=50; total time=   1.0s\n",
      "[CV] END criterion=squared_error, max_depth=8, min_samples_split=2, n_estimators=50; total time=   1.0s\n",
      "[CV] END criterion=squared_error, max_depth=8, min_samples_split=2, n_estimators=50; total time=   1.0s\n",
      "[CV] END criterion=squared_error, max_depth=8, min_samples_split=2, n_estimators=50; total time=   1.0s\n",
      "[CV] END criterion=squared_error, max_depth=8, min_samples_split=2, n_estimators=50; total time=   1.0s\n",
      "[CV] END criterion=squared_error, max_depth=8, min_samples_split=2, n_estimators=100; total time=   1.9s\n",
      "[CV] END criterion=squared_error, max_depth=8, min_samples_split=2, n_estimators=100; total time=   1.9s\n",
      "[CV] END criterion=squared_error, max_depth=8, min_samples_split=2, n_estimators=100; total time=   1.9s\n",
      "[CV] END criterion=squared_error, max_depth=8, min_samples_split=2, n_estimators=100; total time=   1.9s\n",
      "[CV] END criterion=squared_error, max_depth=8, min_samples_split=2, n_estimators=100; total time=   2.0s\n",
      "[CV] END criterion=squared_error, max_depth=8, min_samples_split=2, n_estimators=200; total time=   4.1s\n",
      "[CV] END criterion=squared_error, max_depth=8, min_samples_split=2, n_estimators=200; total time=   3.9s\n",
      "[CV] END criterion=squared_error, max_depth=8, min_samples_split=2, n_estimators=200; total time=   3.8s\n",
      "[CV] END criterion=squared_error, max_depth=8, min_samples_split=2, n_estimators=200; total time=   3.9s\n",
      "[CV] END criterion=squared_error, max_depth=8, min_samples_split=2, n_estimators=200; total time=   3.9s\n",
      "[CV] END criterion=squared_error, max_depth=8, min_samples_split=0.1, n_estimators=10; total time=   0.1s\n",
      "[CV] END criterion=squared_error, max_depth=8, min_samples_split=0.1, n_estimators=10; total time=   0.1s\n",
      "[CV] END criterion=squared_error, max_depth=8, min_samples_split=0.1, n_estimators=10; total time=   0.1s\n",
      "[CV] END criterion=squared_error, max_depth=8, min_samples_split=0.1, n_estimators=10; total time=   0.1s\n",
      "[CV] END criterion=squared_error, max_depth=8, min_samples_split=0.1, n_estimators=10; total time=   0.1s\n",
      "[CV] END criterion=squared_error, max_depth=8, min_samples_split=0.1, n_estimators=50; total time=   0.6s\n",
      "[CV] END criterion=squared_error, max_depth=8, min_samples_split=0.1, n_estimators=50; total time=   0.5s\n",
      "[CV] END criterion=squared_error, max_depth=8, min_samples_split=0.1, n_estimators=50; total time=   0.5s\n",
      "[CV] END criterion=squared_error, max_depth=8, min_samples_split=0.1, n_estimators=50; total time=   0.6s\n",
      "[CV] END criterion=squared_error, max_depth=8, min_samples_split=0.1, n_estimators=50; total time=   0.5s\n",
      "[CV] END criterion=squared_error, max_depth=8, min_samples_split=0.1, n_estimators=100; total time=   1.0s\n",
      "[CV] END criterion=squared_error, max_depth=8, min_samples_split=0.1, n_estimators=100; total time=   1.0s\n",
      "[CV] END criterion=squared_error, max_depth=8, min_samples_split=0.1, n_estimators=100; total time=   1.0s\n",
      "[CV] END criterion=squared_error, max_depth=8, min_samples_split=0.1, n_estimators=100; total time=   1.0s\n",
      "[CV] END criterion=squared_error, max_depth=8, min_samples_split=0.1, n_estimators=100; total time=   1.0s\n",
      "[CV] END criterion=squared_error, max_depth=8, min_samples_split=0.1, n_estimators=200; total time=   2.1s\n",
      "[CV] END criterion=squared_error, max_depth=8, min_samples_split=0.1, n_estimators=200; total time=   2.0s\n",
      "[CV] END criterion=squared_error, max_depth=8, min_samples_split=0.1, n_estimators=200; total time=   2.0s\n",
      "[CV] END criterion=squared_error, max_depth=8, min_samples_split=0.1, n_estimators=200; total time=   2.1s\n",
      "[CV] END criterion=squared_error, max_depth=8, min_samples_split=0.1, n_estimators=200; total time=   2.0s\n",
      "[CV] END criterion=squared_error, max_depth=8, min_samples_split=0.3, n_estimators=10; total time=   0.1s\n",
      "[CV] END criterion=squared_error, max_depth=8, min_samples_split=0.3, n_estimators=10; total time=   0.1s\n",
      "[CV] END criterion=squared_error, max_depth=8, min_samples_split=0.3, n_estimators=10; total time=   0.1s\n",
      "[CV] END criterion=squared_error, max_depth=8, min_samples_split=0.3, n_estimators=10; total time=   0.1s\n",
      "[CV] END criterion=squared_error, max_depth=8, min_samples_split=0.3, n_estimators=10; total time=   0.1s\n",
      "[CV] END criterion=squared_error, max_depth=8, min_samples_split=0.3, n_estimators=50; total time=   0.3s\n",
      "[CV] END criterion=squared_error, max_depth=8, min_samples_split=0.3, n_estimators=50; total time=   0.3s\n",
      "[CV] END criterion=squared_error, max_depth=8, min_samples_split=0.3, n_estimators=50; total time=   0.3s\n",
      "[CV] END criterion=squared_error, max_depth=8, min_samples_split=0.3, n_estimators=50; total time=   0.3s\n",
      "[CV] END criterion=squared_error, max_depth=8, min_samples_split=0.3, n_estimators=50; total time=   0.3s\n",
      "[CV] END criterion=squared_error, max_depth=8, min_samples_split=0.3, n_estimators=100; total time=   0.6s\n",
      "[CV] END criterion=squared_error, max_depth=8, min_samples_split=0.3, n_estimators=100; total time=   0.5s\n",
      "[CV] END criterion=squared_error, max_depth=8, min_samples_split=0.3, n_estimators=100; total time=   0.5s\n",
      "[CV] END criterion=squared_error, max_depth=8, min_samples_split=0.3, n_estimators=100; total time=   0.6s\n",
      "[CV] END criterion=squared_error, max_depth=8, min_samples_split=0.3, n_estimators=100; total time=   0.5s\n",
      "[CV] END criterion=squared_error, max_depth=8, min_samples_split=0.3, n_estimators=200; total time=   1.3s\n",
      "[CV] END criterion=squared_error, max_depth=8, min_samples_split=0.3, n_estimators=200; total time=   1.0s\n",
      "[CV] END criterion=squared_error, max_depth=8, min_samples_split=0.3, n_estimators=200; total time=   1.0s\n",
      "[CV] END criterion=squared_error, max_depth=8, min_samples_split=0.3, n_estimators=200; total time=   1.2s\n",
      "[CV] END criterion=squared_error, max_depth=8, min_samples_split=0.3, n_estimators=200; total time=   1.0s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_split=2, n_estimators=10; total time=   0.5s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_split=2, n_estimators=10; total time=   0.5s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_split=2, n_estimators=10; total time=   0.5s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_split=2, n_estimators=10; total time=   0.5s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_split=2, n_estimators=10; total time=   0.5s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_split=2, n_estimators=50; total time=   2.6s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_split=2, n_estimators=50; total time=   2.5s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_split=2, n_estimators=50; total time=   2.5s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_split=2, n_estimators=50; total time=   2.6s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_split=2, n_estimators=50; total time=   2.6s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_split=2, n_estimators=100; total time=   5.1s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_split=2, n_estimators=100; total time=   5.0s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_split=2, n_estimators=100; total time=   4.9s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_split=2, n_estimators=100; total time=   5.1s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_split=2, n_estimators=100; total time=   5.0s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_split=2, n_estimators=200; total time=  10.4s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_split=2, n_estimators=200; total time=   9.8s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_split=2, n_estimators=200; total time=   9.9s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_split=2, n_estimators=200; total time=  10.0s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_split=2, n_estimators=200; total time=  10.0s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_split=0.1, n_estimators=10; total time=   0.1s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_split=0.1, n_estimators=10; total time=   0.1s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_split=0.1, n_estimators=10; total time=   0.1s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_split=0.1, n_estimators=10; total time=   0.1s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_split=0.1, n_estimators=10; total time=   0.1s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_split=0.1, n_estimators=50; total time=   0.5s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_split=0.1, n_estimators=50; total time=   0.5s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_split=0.1, n_estimators=50; total time=   0.5s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_split=0.1, n_estimators=50; total time=   0.5s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_split=0.1, n_estimators=50; total time=   0.5s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_split=0.1, n_estimators=100; total time=   1.0s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_split=0.1, n_estimators=100; total time=   0.9s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_split=0.1, n_estimators=100; total time=   0.9s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_split=0.1, n_estimators=100; total time=   1.0s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_split=0.1, n_estimators=100; total time=   0.9s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_split=0.1, n_estimators=200; total time=   2.0s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_split=0.1, n_estimators=200; total time=   1.8s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_split=0.1, n_estimators=200; total time=   1.9s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_split=0.1, n_estimators=200; total time=   2.0s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_split=0.1, n_estimators=200; total time=   1.9s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_split=0.3, n_estimators=10; total time=   0.1s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_split=0.3, n_estimators=10; total time=   0.1s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_split=0.3, n_estimators=10; total time=   0.1s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_split=0.3, n_estimators=10; total time=   0.1s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_split=0.3, n_estimators=10; total time=   0.1s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_split=0.3, n_estimators=50; total time=   0.3s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_split=0.3, n_estimators=50; total time=   0.2s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_split=0.3, n_estimators=50; total time=   0.2s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_split=0.3, n_estimators=50; total time=   0.3s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_split=0.3, n_estimators=50; total time=   0.3s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_split=0.3, n_estimators=100; total time=   0.6s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_split=0.3, n_estimators=100; total time=   0.5s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_split=0.3, n_estimators=100; total time=   0.5s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_split=0.3, n_estimators=100; total time=   0.6s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_split=0.3, n_estimators=100; total time=   0.5s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_split=0.3, n_estimators=200; total time=   1.2s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_split=0.3, n_estimators=200; total time=   1.0s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_split=0.3, n_estimators=200; total time=   0.9s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_split=0.3, n_estimators=200; total time=   1.2s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_split=0.3, n_estimators=200; total time=   1.0s\n",
      "[CV] END criterion=absolute_error, max_depth=2, min_samples_split=2, n_estimators=10; total time=  10.6s\n",
      "[CV] END criterion=absolute_error, max_depth=2, min_samples_split=2, n_estimators=10; total time=  10.3s\n",
      "[CV] END criterion=absolute_error, max_depth=2, min_samples_split=2, n_estimators=10; total time=   9.9s\n",
      "[CV] END criterion=absolute_error, max_depth=2, min_samples_split=2, n_estimators=10; total time=  10.4s\n",
      "[CV] END criterion=absolute_error, max_depth=2, min_samples_split=2, n_estimators=10; total time=  10.7s\n",
      "[CV] END criterion=absolute_error, max_depth=2, min_samples_split=2, n_estimators=50; total time=  53.5s\n",
      "[CV] END criterion=absolute_error, max_depth=2, min_samples_split=2, n_estimators=50; total time=  50.4s\n",
      "[CV] END criterion=absolute_error, max_depth=2, min_samples_split=2, n_estimators=50; total time=  46.7s\n",
      "[CV] END criterion=absolute_error, max_depth=2, min_samples_split=2, n_estimators=50; total time=  48.2s\n",
      "[CV] END criterion=absolute_error, max_depth=2, min_samples_split=2, n_estimators=50; total time=  49.0s\n",
      "[CV] END criterion=absolute_error, max_depth=2, min_samples_split=2, n_estimators=100; total time= 1.7min\n",
      "[CV] END criterion=absolute_error, max_depth=2, min_samples_split=2, n_estimators=100; total time= 1.6min\n",
      "[CV] END criterion=absolute_error, max_depth=2, min_samples_split=2, n_estimators=100; total time= 1.5min\n",
      "[CV] END criterion=absolute_error, max_depth=2, min_samples_split=2, n_estimators=100; total time= 1.5min\n",
      "[CV] END criterion=absolute_error, max_depth=2, min_samples_split=2, n_estimators=100; total time= 1.6min\n",
      "[CV] END criterion=absolute_error, max_depth=2, min_samples_split=2, n_estimators=200; total time= 3.2min\n",
      "[CV] END criterion=absolute_error, max_depth=2, min_samples_split=2, n_estimators=200; total time= 3.2min\n",
      "[CV] END criterion=absolute_error, max_depth=2, min_samples_split=2, n_estimators=200; total time= 3.0min\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/work/code/DataSci-AU-24/code_MA/class_05.ipynb Cell 11\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://app-5024230-0.cloud.sdu.dk/work/code/DataSci-AU-24/code_MA/class_05.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# fit \u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://app-5024230-0.cloud.sdu.dk/work/code/DataSci-AU-24/code_MA/class_05.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m grid\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n",
      "File \u001b[0;32m/work/code/DataSci-AU-24/env/lib/python3.10/site-packages/sklearn/base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1467\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1469\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1470\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m ):\n\u001b[0;32m-> 1474\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/work/code/DataSci-AU-24/env/lib/python3.10/site-packages/sklearn/model_selection/_search.py:970\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    964\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[1;32m    965\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    966\u001b[0m     )\n\u001b[1;32m    968\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[0;32m--> 970\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[1;32m    972\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    973\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    974\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m/work/code/DataSci-AU-24/env/lib/python3.10/site-packages/sklearn/model_selection/_search.py:1527\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1525\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1526\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1527\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_grid))\n",
      "File \u001b[0;32m/work/code/DataSci-AU-24/env/lib/python3.10/site-packages/sklearn/model_selection/_search.py:916\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    909\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[1;32m    910\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFitting \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m folds for each of \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m candidates,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    911\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m totalling \u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m fits\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    912\u001b[0m             n_splits, n_candidates, n_candidates \u001b[39m*\u001b[39m n_splits\n\u001b[1;32m    913\u001b[0m         )\n\u001b[1;32m    914\u001b[0m     )\n\u001b[0;32m--> 916\u001b[0m out \u001b[39m=\u001b[39m parallel(\n\u001b[1;32m    917\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m    918\u001b[0m         clone(base_estimator),\n\u001b[1;32m    919\u001b[0m         X,\n\u001b[1;32m    920\u001b[0m         y,\n\u001b[1;32m    921\u001b[0m         train\u001b[39m=\u001b[39;49mtrain,\n\u001b[1;32m    922\u001b[0m         test\u001b[39m=\u001b[39;49mtest,\n\u001b[1;32m    923\u001b[0m         parameters\u001b[39m=\u001b[39;49mparameters,\n\u001b[1;32m    924\u001b[0m         split_progress\u001b[39m=\u001b[39;49m(split_idx, n_splits),\n\u001b[1;32m    925\u001b[0m         candidate_progress\u001b[39m=\u001b[39;49m(cand_idx, n_candidates),\n\u001b[1;32m    926\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_and_score_kwargs,\n\u001b[1;32m    927\u001b[0m     )\n\u001b[1;32m    928\u001b[0m     \u001b[39mfor\u001b[39;49;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[39min\u001b[39;49;00m product(\n\u001b[1;32m    929\u001b[0m         \u001b[39menumerate\u001b[39;49m(candidate_params),\n\u001b[1;32m    930\u001b[0m         \u001b[39menumerate\u001b[39;49m(cv\u001b[39m.\u001b[39;49msplit(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mrouted_params\u001b[39m.\u001b[39;49msplitter\u001b[39m.\u001b[39;49msplit)),\n\u001b[1;32m    931\u001b[0m     )\n\u001b[1;32m    932\u001b[0m )\n\u001b[1;32m    934\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    935\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    936\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo fits were performed. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    937\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWas the CV iterator empty? \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    938\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWere there no candidates?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    939\u001b[0m     )\n",
      "File \u001b[0;32m/work/code/DataSci-AU-24/env/lib/python3.10/site-packages/sklearn/utils/parallel.py:67\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     62\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[1;32m     63\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[1;32m     64\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     65\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[1;32m     66\u001b[0m )\n\u001b[0;32m---> 67\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[0;32m/work/code/DataSci-AU-24/env/lib/python3.10/site-packages/joblib/parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1861\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1862\u001b[0m     \u001b[39mnext\u001b[39m(output)\n\u001b[0;32m-> 1863\u001b[0m     \u001b[39mreturn\u001b[39;00m output \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_generator \u001b[39melse\u001b[39;00m \u001b[39mlist\u001b[39;49m(output)\n\u001b[1;32m   1865\u001b[0m \u001b[39m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1866\u001b[0m \u001b[39m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1867\u001b[0m \u001b[39m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1868\u001b[0m \u001b[39m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1869\u001b[0m \u001b[39m# callback.\u001b[39;00m\n\u001b[1;32m   1870\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n",
      "File \u001b[0;32m/work/code/DataSci-AU-24/env/lib/python3.10/site-packages/joblib/parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1790\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_dispatched_batches \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1791\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_dispatched_tasks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m-> 1792\u001b[0m res \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1793\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_completed_tasks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1794\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m/work/code/DataSci-AU-24/env/lib/python3.10/site-packages/sklearn/utils/parallel.py:129\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    127\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[1;32m    128\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[0;32m--> 129\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/work/code/DataSci-AU-24/env/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:895\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, score_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    893\u001b[0m         estimator\u001b[39m.\u001b[39mfit(X_train, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[1;32m    894\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 895\u001b[0m         estimator\u001b[39m.\u001b[39;49mfit(X_train, y_train, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\n\u001b[1;32m    897\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m    898\u001b[0m     \u001b[39m# Note fit time as time until error\u001b[39;00m\n\u001b[1;32m    899\u001b[0m     fit_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n",
      "File \u001b[0;32m/work/code/DataSci-AU-24/env/lib/python3.10/site-packages/sklearn/base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1467\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1469\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1470\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m ):\n\u001b[0;32m-> 1474\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/work/code/DataSci-AU-24/env/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:489\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    478\u001b[0m trees \u001b[39m=\u001b[39m [\n\u001b[1;32m    479\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_estimator(append\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, random_state\u001b[39m=\u001b[39mrandom_state)\n\u001b[1;32m    480\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_more_estimators)\n\u001b[1;32m    481\u001b[0m ]\n\u001b[1;32m    483\u001b[0m \u001b[39m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[1;32m    484\u001b[0m \u001b[39m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[1;32m    485\u001b[0m \u001b[39m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[1;32m    486\u001b[0m \u001b[39m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[1;32m    487\u001b[0m \u001b[39m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[1;32m    488\u001b[0m \u001b[39m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[0;32m--> 489\u001b[0m trees \u001b[39m=\u001b[39m Parallel(\n\u001b[1;32m    490\u001b[0m     n_jobs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_jobs,\n\u001b[1;32m    491\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[1;32m    492\u001b[0m     prefer\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mthreads\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    493\u001b[0m )(\n\u001b[1;32m    494\u001b[0m     delayed(_parallel_build_trees)(\n\u001b[1;32m    495\u001b[0m         t,\n\u001b[1;32m    496\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbootstrap,\n\u001b[1;32m    497\u001b[0m         X,\n\u001b[1;32m    498\u001b[0m         y,\n\u001b[1;32m    499\u001b[0m         sample_weight,\n\u001b[1;32m    500\u001b[0m         i,\n\u001b[1;32m    501\u001b[0m         \u001b[39mlen\u001b[39;49m(trees),\n\u001b[1;32m    502\u001b[0m         verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[1;32m    503\u001b[0m         class_weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclass_weight,\n\u001b[1;32m    504\u001b[0m         n_samples_bootstrap\u001b[39m=\u001b[39;49mn_samples_bootstrap,\n\u001b[1;32m    505\u001b[0m         missing_values_in_feature_mask\u001b[39m=\u001b[39;49mmissing_values_in_feature_mask,\n\u001b[1;32m    506\u001b[0m     )\n\u001b[1;32m    507\u001b[0m     \u001b[39mfor\u001b[39;49;00m i, t \u001b[39min\u001b[39;49;00m \u001b[39menumerate\u001b[39;49m(trees)\n\u001b[1;32m    508\u001b[0m )\n\u001b[1;32m    510\u001b[0m \u001b[39m# Collect newly grown trees\u001b[39;00m\n\u001b[1;32m    511\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimators_\u001b[39m.\u001b[39mextend(trees)\n",
      "File \u001b[0;32m/work/code/DataSci-AU-24/env/lib/python3.10/site-packages/sklearn/utils/parallel.py:67\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     62\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[1;32m     63\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[1;32m     64\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     65\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[1;32m     66\u001b[0m )\n\u001b[0;32m---> 67\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[0;32m/work/code/DataSci-AU-24/env/lib/python3.10/site-packages/joblib/parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1861\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1862\u001b[0m     \u001b[39mnext\u001b[39m(output)\n\u001b[0;32m-> 1863\u001b[0m     \u001b[39mreturn\u001b[39;00m output \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_generator \u001b[39melse\u001b[39;00m \u001b[39mlist\u001b[39;49m(output)\n\u001b[1;32m   1865\u001b[0m \u001b[39m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1866\u001b[0m \u001b[39m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1867\u001b[0m \u001b[39m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1868\u001b[0m \u001b[39m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1869\u001b[0m \u001b[39m# callback.\u001b[39;00m\n\u001b[1;32m   1870\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n",
      "File \u001b[0;32m/work/code/DataSci-AU-24/env/lib/python3.10/site-packages/joblib/parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1790\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_dispatched_batches \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1791\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_dispatched_tasks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m-> 1792\u001b[0m res \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1793\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_completed_tasks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1794\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m/work/code/DataSci-AU-24/env/lib/python3.10/site-packages/sklearn/utils/parallel.py:129\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    127\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[1;32m    128\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[0;32m--> 129\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/work/code/DataSci-AU-24/env/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:192\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap, missing_values_in_feature_mask)\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[39melif\u001b[39;00m class_weight \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbalanced_subsample\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    190\u001b[0m         curr_sample_weight \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m compute_sample_weight(\u001b[39m\"\u001b[39m\u001b[39mbalanced\u001b[39m\u001b[39m\"\u001b[39m, y, indices\u001b[39m=\u001b[39mindices)\n\u001b[0;32m--> 192\u001b[0m     tree\u001b[39m.\u001b[39;49m_fit(\n\u001b[1;32m    193\u001b[0m         X,\n\u001b[1;32m    194\u001b[0m         y,\n\u001b[1;32m    195\u001b[0m         sample_weight\u001b[39m=\u001b[39;49mcurr_sample_weight,\n\u001b[1;32m    196\u001b[0m         check_input\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    197\u001b[0m         missing_values_in_feature_mask\u001b[39m=\u001b[39;49mmissing_values_in_feature_mask,\n\u001b[1;32m    198\u001b[0m     )\n\u001b[1;32m    199\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    200\u001b[0m     tree\u001b[39m.\u001b[39m_fit(\n\u001b[1;32m    201\u001b[0m         X,\n\u001b[1;32m    202\u001b[0m         y,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    205\u001b[0m         missing_values_in_feature_mask\u001b[39m=\u001b[39mmissing_values_in_feature_mask,\n\u001b[1;32m    206\u001b[0m     )\n",
      "File \u001b[0;32m/work/code/DataSci-AU-24/env/lib/python3.10/site-packages/sklearn/tree/_classes.py:472\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[0;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    462\u001b[0m     builder \u001b[39m=\u001b[39m BestFirstTreeBuilder(\n\u001b[1;32m    463\u001b[0m         splitter,\n\u001b[1;32m    464\u001b[0m         min_samples_split,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmin_impurity_decrease,\n\u001b[1;32m    470\u001b[0m     )\n\u001b[0;32m--> 472\u001b[0m builder\u001b[39m.\u001b[39;49mbuild(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtree_, X, y, sample_weight, missing_values_in_feature_mask)\n\u001b[1;32m    474\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_outputs_ \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m is_classifier(\u001b[39mself\u001b[39m):\n\u001b[1;32m    475\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_[\u001b[39m0\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# fit \n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'ccp_alpha': 0.0,\n",
       " 'criterion': 'squared_error',\n",
       " 'max_depth': 8,\n",
       " 'max_features': 1.0,\n",
       " 'max_leaf_nodes': None,\n",
       " 'max_samples': None,\n",
       " 'min_impurity_decrease': 0.0,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 0.1,\n",
       " 'min_weight_fraction_leaf': 0.0,\n",
       " 'monotonic_cst': None,\n",
       " 'n_estimators': 50,\n",
       " 'n_jobs': None,\n",
       " 'oob_score': False,\n",
       " 'random_state': 3,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract best estimator\n",
    "grid.best_estimator_.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_estimator = grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performances = evaluate_across_splits(model=best_estimator, \n",
    "                                            X_splits=[X_train, X_val, X_test],\n",
    "                                            y_splits=[y_train, y_val, y_test],\n",
    "                                            split_names=[\"train\", \"val\", \"test\"],\n",
    "                                            model_name=\"randomforest\",\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'model': 'randomforest', 'split': 'train', 'rmse': 45.176, 'r2': 0.6623},\n",
       " {'model': 'randomforest', 'split': 'val', 'rmse': 59.7168, 'r2': 0.3896},\n",
       " {'model': 'randomforest', 'split': 'test', 'rmse': 55.8289, 'r2': 0.4294}]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performances"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
